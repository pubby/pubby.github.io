<!doctype html>
<html lang="en-us">
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="style.css">
        <h1>Explaining my fast 6502 code generator</h1>
    </head>
    <body>
        <p>
        To learn how optimizing compilers are made, I built one targeting the <a href="https://en.wikipedia.org/wiki/MOS_Technology_6502">6502 architecture</a>.
        In a bizarre twist, my compiler generates faster code than GCC, LLVM, and every other compiler I compared it to.
        </p>

        <img src="nesfab/combined.png">

        <p>I reckon my compiler isn't doing more when it comes to high-level optimizations,
        so the gains must be from the <a href="https://en.wikipedia.org/wiki/Code_generation_(compiler)">code generation</a> side.
        This makes sense, as most compilers are multi-target with backends
        designed for modern out-of-order RISC-like systems, not the ancient 6502.
        It doesn't matter how good GCC or LLVM's high-level optimizations are
        if they falter at the last leg of the race.
        </p>

        <p>Still, my compiler also beats those designed for retro and embedded systems, like VBCC, SDCC, and KickC.
        For this reason, it seemed like a good idea to write about my technique.</p>

        <h2>Cheating Disclaimer</h2>

        <p>Before I get into it, I want to cover three areas my compiler has an advantage, which aren't tied to algorithmic design.
        I call these my "cheats", as other compilers <i>could</i> do these things, but there are trade-offs involved.</p>

        <p>
        First, my compiler generates what are know as
        <a href="https://en.wikipedia.org/wiki/Illegal_opcode">"illegal" instructions</a>.
        An illegal instruction is one which isn't officially documented by the manufacturer,
        but still exists in the hardware. 
        On most 6502 chips, a few illegal instructions exist which combine the behavior of two "legal" instructions.
        Their use can save a few cycles, but not every backend generates them.
        </p>

        <p>Second, some of my compiler's gains can be explained by its computational effort.
        My compiler spends the majority of its CPU budget (a few milliseconds) doing instruction selection,
        but not all compilers do. 
        Typically, the more time spent searching for a solution, the better the results.
        </p>

        <p>
        Lastly, there is loop unrolling and other optimizations that trade space efficiency for speed,
        which is hard to compare fairly when benchmarking different compilers.
        I tried to pick tests which weren't affected by this, 
        but obviously I can't be perfect. 
        </p>

        <p>With my conscience clear, onto the algorithm!</p>

        <h2>A Quick, Vague Overview</h2>

        <p>
        I've been calling my own attempt "<a href="https://en.wikipedia.org/wiki/Outsider_art">outsider art</a>",
        as I didn't know much about code generation when writing it.
        In fact, the technique didn't even originate in my compiler;
        I based it on an old animation system I wrote years prior.
        My algorithm is interesting because it combines instruction selection
        with register allocation, but also because it's written using <a href="https://en.wikipedia.org/wiki/Continuation">continuations</a>.
        In all my years of programming, it's the only practical use I've found for such black magic.
        </p>

        <p>
        To briefly explain how it works, I'll say that each basic block in my 
        <a href="https://en.wikipedia.org/wiki/Intermediate_representation">IR</a> is represented as a
        <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a> in 
        <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">SSA form</a>.
        The first step of code generation is to break both of these properties,
        converting basic blocks to <a href="https://en.wikipedia.org/wiki/Total_order">totally-ordered</a> lists that do not contain phi nodes.
        </p>

        <p>
        The ordered basic block are then individually processed from the beginning of their order to the end,
        generating multiple assembly code combinations per operation.
        The list of combinations will grow at an exponential rate, 
        but most can be pruned using ideas from <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a> 
        and <a href="https://en.wikipedia.org/wiki/Branch_and_bound">branch-and-bound</a>.
        </p>

        <p>
        To improve the generated code, the resulting output states of each basic block are fed into their successor blocks as inputs.
        This process repeats until a fixed point is reached.
        A cooling schedule a-la <a href="https://en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</a> 
        is applied to find results faster.
        </p>

        <p>
        After this, each basic block will have multiple code sequences to choose from.
        The most expensive ones can be heuristically pruned, 
        then a good selection can be made by solving a <a href="https://beza1e1.tuxen.de/articles/pbqp.html">partitioned boolean quadratic problem</a> (PBQP).
        </p>

        <p>
        Finally, additional instructions are inserted as transitions between the basic blocks,
        and a few optimization passes are run on the resulting assembly code for good measure.
        </p>

        <hr>

        <h2>A More Detailed Explanation</h2>

        <h3>Basic Block IR</h3>

        <p>As stated, the 
        <a href="https://en.wikipedia.org/wiki/Intermediate_representation">IR</a> 
        of each basic block is a 
        <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a> in 
        in <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">SSA form</a>,
        which sounds complicated until you see a picture of it.
        </p>
        <p>Given the code below:</p>

        <pre>
foo = fn() ^ 5
return foo - (foo &amp; 3)</pre>
        <p>The IR DAG would look like this:</p>
        <img src="codegen/dag.png">
        <p>Each node in the graph represents a value, with edges denoting the flow of data.
        To compute a value, the nodes pointing to it must be computed first &mdash;
        e.g. to compute the node labeled <code>(&amp;)</code>, the nodes <code>(^)</code> and <code>(3)</code> must be computed first.
        It's a <a href="https://en.wikipedia.org/wiki/Dependency_graph">dependency graph</a>.
        </p>


        <h3>Ordering</h3>

        <p>The first step of code generation is to take the IR and create a 
        <a href="https://en.wikipedia.org/wiki/Total_order">total order</a>
        out of its nodes, positioning each node to occur after its dependencies.
        This is akin to putting each node on a number line such that every edge points in the same direction (downwards in the diagram).</p>

        <img src="codegen/scheduled.png">

        <p>The generated assembly code will follow this order.
        First the instructions for <code>(call fn)</code> will execute, then <code>(5)</code>, and so on.</p>

        <h4>How does one find this order?</h4>

        <p>It's easy to do using a <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological sort</a>,
        but there's a problem: DAGs can have multiple valid orderings, and some will generate better code than others.</p>

        <p>To find a good ordering (one which generates efficient code), 
        the topological sorting algorithm is still used, but it can be modified using a few heuristics.
        First, "fake" edges can be added to the graph to constrain nodes to occur after other nodes.
        Second, a greedy algorithm can be used to influence the traversal order of the topological sort.
        In my compiler, I prioritize nodes which form the 
        <a href="https://en.wikipedia.org/wiki/Longest_path_problem">longest path</a> through the graph,
        which produces an ordering that has small
        <a href="https://en.wikipedia.org/wiki/Live-variable_analysis">live ranges</a>,
        resulting in less register pressure and stores.
        </p>

        <img src="codegen/dag2.png">
        <br><sup>Here's an actual DAG from my compiler.</sup>

        <hr>

        <h3>Out-of-SSA</h3>

        <p>Next, the IR will be converted out of <a href="https://en.wikipedia.org/wiki/Static_single-assignment_form">SSA form</a>
        (to a form with variable re-assignment).
        This is done in a standard textbook fashion, but I'll still provide a short explanation.
        <b>Feel free to skip this section if you're not writing a compiler.</b>
        </p>

        <p>To convert out of SSA form, take each phi node in the IR and replace them with copies, 
        then take their inputs and insert copies for those too.
        Using live variable analysis, many of these copies can be coalesced,
        reducing the amount of variables needed.</p>

        <p>For example, let's say you started with this input program:</p>

        <pre>
if some_condition
    X = foo()
else
    X = bar()
print X</pre>

        <p>Converting this program to SSA form mimics addding variable names so that every assignment has a unique name.
        A phi node will have to be inserted at the point where control flow merges together.</p>

        <pre>
// This is in SSA form:
if some_condition
    X = foo()
else
    Y = bar()
P = phi(X, Y)
print P</pre>

        <p>Once this SSA form has been optimized, it then has to be converted back.
        This is where adding copies comes into play.
        Each phi input will be copied to a new common variable before the control flow merges together.
        Then, the phi itself will be replaced with a copy of the new input copies. Take a look:</p>

        <pre>
// Copies inserted:
if some_condition
    X = foo()
    C = X      // Copy
else
    Y = bar()
    C = Y      // Copy
P = C          // Copy
print P</pre>

        <p>Lastly, live variable analysis is run, and each assignment is checked to see if they can use the same variables.
        The steps for this example might resemble:</p>

        <ol>
            <li>Prove variables P and C can use the same variable.</li>
            <li>Prove variables C and X can use the same variable.</li>
            <li>Prove variables X and Y can use the same variable.</li>
        </ol>

        <p>With this information, the final code can be rewritten:</p>

        <pre>
// This is out-of-SSA form:
if some_condition
    X = foo()
    X = X
else
    X = bar()
    X = X
X = X
print X</pre>

        <p>The "X = X" lines can either be removed, or ignored. They have no effect.</p>

        <hr>

        <h3>Basic Block Instruction Selection - Part 1: Animation</h3>

        <p>With the IR prepared, now it's time to generate some assembly code.
        I mentioned earlier that my algorithm was based on an animation system,
        and although that sounds weird and unrelated, it will be easier for me
        to explain the animation system first and then extend it to work like my compiler.</p>

        <p>The animation I'm talking about was for a NES game.
        On that system, animation can be done by loading a new tileset into video memory each frame,
        overwriting the previous frame's tileset.
        </p>

        <img src="codegen/anim.jpg">

        <p>Most games accomplish this using special hardware on the cartridge called a mapper,
        which lets the system page a tileset into video memory quickly.
        But in my case, I was making a game that wasn't going to use that piece of hardware.
        Instead, I had to copy the bytes using the CPU as a middle-man.
        </p>

        <p>The obvious way to copy bytes is with a loop (think "memcpy" from C),
        but this turned out to be too slow.
        For each iteration, too much time was being spent incrementing, then comparing,
        then branching back to the loop.</p>

        <p>The only way I could see it working was if I unrolled the loop &mdash; not once &mdash; but fully.
        For example, if I wanted to copy the byte sequence (10, 20, 30) to video memory, I could write assembly code like this:</p>


        <pre>
lda #20      ; Load constant: 20
sta PPUDATA  ; Upload to video RAM

lda #30      ; Load constant: 30
sta PPUDATA  ; Upload to video RAM

lda #40      ; Load constant: 40
sta PPUDATA  ; Upload to video RAM</pre>

        <p>In other words, I was writing code like a noob who quit programming class before learning what a for-loop was.
        But it was fast, and that's what mattered.</p>

        <p>Rather than writing these sequences by hand, it was clear I should automate it.
        A naive approach would be to write a script which generates one load and one store per uploaded byte,
        but it's possible to do better.
        As registers preserve their values, a sequence like (1, 8, 1, 8, 1, 8) requires only two loads:</p>

        <pre>
ldx #1      ; Load register X
lda #8      ; Load register A
stx PPUDATA ; Upload register X to video RAM
sta PPUDATA ; Upload register A to video RAM
stx PPUDATA
sta PPUDATA
stx PPUDATA
sta PPUDATA</pre>

        <p><b>How does one determine these loads optimally?</b></p>

        <p>To keep things simple, I'll first explain how to solve this for a CPU with one register,
        then expand to a CPU with three.</p>

        <p>For a CPU with one register, the algorithm boils down to finding redundant loads and eliding them.
        By redundant load, I mean a load which has no effect, like the second load below:</p>

        <pre>
lda #10
sta PPUDATA
lda #10       // redundant
sta PPUDATA</pre>

        <p>Finding and eliding redundant loads is easy to do; just keep track of the previously loaded value.
        The pseudocode below does just that, optimally:</p>

        <pre>
prev = -1

for each byte to upload:
    if prev != byte:
        emit_load(byte)
        prev = byte

    emit_store()</pre>

        <p>This idea can be extended to working with three registers
        by using three "prev" variables instead of one.
        You might approach it using a struct or record type to keep things clean:</p>
            

        <pre>
struct cpu_state
{
    int a
    int x
    int y
}</pre>

        <p>Unfortunately, there's a problem. 
        Although it's possible to elide loads this way, 
        it doesn't tell us which registers to use for each byte.
        The one-register version works because each byte uses the lone register,
        but for three registers, there's a decision to be made for each byte.</p>

        <p>These decisions can be made optimally by testing every possible combination of registers in a brute-force fashion, 
        but this quickly becomes infeasible.
        For each byte that needs uploading, there are three choices possible (one for each register).
        The number of combinations will grow at an exponential rate.</p>

        <img src="codegen/tree.png">
        <br><sup>Diagram of the brute-force search tree.</sup>

        <p>Luckily, this brute-force approach is still usable if a trick is used.
        <b>It's possible to prune most combinations, shrinking the size of the search space.</b></p>

        <p>If two pieces of assembly code have been generated, and both have the same observable side effect,
        the higher costing one can be pruned. 
        By side effects, I mean that both upload the same byte sequence
        and finish with identical register states.
        For example, these two pieces of code are equivalent:
        </p>

<pre>
lda #1      ; Load register A
sta PPUDATA ; Upload register A to video RAM
sta PPUDATA ; Upload register A to video RAM
ldx #2      ; Load register X
stx PPUDATA ; Upload register X to video RAM
ldy #3      ; Load register Y
sty PPUDATA ; Upload register Y to video RAM
</pre>


<pre>
ldx #1      ; Load register X
stx PPUDATA ; Upload register X to video RAM
lda #1      ; Load register A
sta PPUDATA ; Upload register A to video RAM
ldx #2      ; Load register X
stx PPUDATA ; Upload register X to video RAM
ldy #3      ; Load register Y
sty PPUDATA ; Upload register Y to video RAM
</pre>

<p>Both upload the sequence (1, 1, 2, 3), and both leave the registers at (A = 1, X = 2, Y = 3).
The second one requires an extra load though, so it can be pruned.</p>

<p>Keep in mind that this works for partial snippets of code too.
Instead generating entire code sequences and comparing them,
it's possible to generate the code instruction-by-instruction and compare at each step.</p>

<p>To implement this, the algorithm will handle the combinations in a breadth-first fashion.
It will first generate all the combinations for the first byte and prune some,
then it will generate all the combinations for the second byte and prune some, and so on.
The pruning is done by storing the results in a hash table, indexed by each combination's "cpu_state"
(the struct introduced earlier).</p>

<p>At the end of the algorithm, several combinations will exist.
The solution will be the lowest-costing combination of the bunch.</p>

<p>Here's pseudocode:</p>

<pre>
for each byte to upload:
    next_hash_map.clear()

    for each combination in prev_hash_map:
        for each register (A, X, Y):
            new_combination = combination
            if new_combination.cpu_state.register != byte:
                Append a load to new_combination.code
                new_combination.cost += 1
                new_combination.cpu_state.register = byte

            next_hash_map.insert(new_combination)
            if next_hash_map already had this cpu_state: 
                Keep the lowest-costing one

    swap(next_hash_map,  prev_hash_map)</pre>

<p>For better performance, there are more ways we can prune.
One good method is to keep track of the current best (lowest-costing) combination,
and prune other combinations that are slower by three or more loads.
This preserves optimality, as three loads is enough to convert from any "cpu_state" to another.
</p>

<p>Applying this, the pseudocode becomes:</p>

<pre>
for each byte to upload:
    next_lowest_cost = INFINITY
    next_hash_map.clear()

    for each combination in prev_hash_map:
        if combination.cost &gt;= prev_lowest_cost + 3:
            continue

        for each register (A, X, Y):
            new_combination = combination
            if new_combination.cpu_state.register != byte:
                Append a load to new_combination.code
                new_combination.cost += 1
                new_combination.cpu_state.register = byte

            if combination.cost &lt; next_lowest_cost:
                next_lowest_cost = combination.cost

            next_hash_map.insert(new_combination)
            if next_hash_map already had this cpu_state: 
                Keep the lowest-costing one

    swap(next_hash_map, prev_hash_map)
    prev_lowest_cost = next_lowest_cost</pre>

<p>(If you want to see actual code, I've put a C++ implementation 
<a href="https://coliru.stacked-crooked.com/a/d9b6528f72b870a1">here</a>
and <a href="https://pastebin.com/raw/HAh3smh1">here</a>.)</p>

<h4>Even More Ways to Prune</h4>

<p>There are more ways to prune, but they're not applicable to how my compiler works.
For instance, instead of comparing cpu_states as 3-tuples, it's better to compare them as sets,
as there's no behavoral differences between the three registers. 
Likewise, if <i>any</i> register in "cpu_state" matches the byte to upload,
there's no point in trying any other register.
But again, these ideas can not be used for my compiler.</p>

<p>There is also a way to prune which <i>is</i> useful to a compiler, but not vice versa.
Once the number combinations surpasses some specified amount (say, 100 or so), 
the highest-costing ones above this threshold are pruned.
This means the final result will not always be optimal, but it ensures that the runtime performance remains linear and predictable.</p>



        <img src="codegen/bebop.gif">
        <br><sup>I converted the Cowboy Bebop intro to a NES ROM using this animation process.
        The GIF runs at a lower framerate.</sup>

        <hr>

        <h3>Basic Block Instruction Selection - Part 2: The Compiler</h3>

        <p>Alright, alright. . .  we can generate code for animations &mdash; So what?
        This is an article about compilers, right?</p>

        <p>
        Well, consider this: <b>Instead of processing a sequence of bytes with this algorithm, 
            process the sequence of IR operations created earlier.</b></p>

        <p>Here's the ordered sequence of IR operations, from earlier.
        I've added variable names (Q, R, S, T):</p>

        <pre>
Q = call fn
R = Q ^ 5
S = R &amp; 3
T = R - S
return T</pre>

        <p>Like we did for the sequence of bytes, we can generate combinations per operation and prune the worst ones,
        but there are two big differences:</p>

        <ol>
            <li>Different IR operations generate different assembly instructions (not just loads).</li>
            <li>The "cpu_state" struct will also track variable names.</li>
        </ol>

        <p>Putting this into practice, I'll show how the algorithm handles the ordered IR.
        To start, it will generate all the combinations for the <b>"call fn"</b> IR operation,
        of which there is only one valid assembly code sequence:</p>

        <pre>
jsr fn   ; Call the function
sta Q    ; Store the result into variable Q
         ; cpu_state is now (A = Q, X = ?, Y = ?)</pre>

        <p>By "generate", I mean it's copy-pasting a pattern containing these instructions,
        like expanding a macro.</p>

        <p>After that, it will generate the combinations for the <b>XOR operation (^)</b>.
        There's a single assembly instruction which does this: "EOR",
        but since XOR is commutative, two code combinations exist:
        </p>

        <pre>
lda #5 ; Load 5 into register A
eor Q  ; XOR register A with variable Q
sta R  ; Store the result into variable R
       ; cpu_state is (A = R, X = ?, Y = ?)</pre>

        <pre>
eor #5 ; XOR register A with variable 5
sta R  ; Store the result into variable R
       ; cpu_state is (A = R, X = ?, Y = ?)</pre>

        <p>The second combination does not need to load the Q variable, as it's already loaded in the register.
        Since both of these code sequences have the same side effect, the first combination will be pruned.</p>

        <p>Now, for the <b>AND operation (&amp;)</b>.
        While there is an AND instruction, there's also an illegal instruction called SAX,
        which perform a bitwise AND of register A with register X and stores the result. 
        This results in four combinations, so I'll only show the two that won't get pruned:
        </p>

        <pre>
and #3   ; AND register A with 5
sta S    ; Store the result into variable S
         ; cpu_state is (A = S, X = ?, Y = ?)</pre>

        <pre>
ldx #3   ; Load 3 into register X
sax S    ; Store the AND of registers A and X into variable S
         ; cpu_state is (A = R, X = 3, Y = ?)</pre>

        <p>For the <b>subtraction operation (-)</b>, the SBC instruction will be used following a SEC instruction to set the carry flag.
        As subtraction isn't commutative, there's only one way to do this, but since the previous operation generated two combinations,
        this operation will result in two combinations too. 
        I'll include the previous AND operation in the code snippets, for clarity.
        </p>

        <pre>
and #3   ; AND register A with 5
sta S    ; Store the result into variable S
         ; cpu_state is (A = S, X = ?, Y = ?)
lda R    ; Load R into register A
sec
sbc S    ; Subtract S from register A
sta T    ; Store the result into variable T
         ; cpu_state is (A = T, X = ?, Y = ?)</pre>

        <pre>
ldx #3   ; Load 3 into register X
sax S    ; Store the AND of registers A and X into variable S
         ; cpu_state is (A = R, X = 3, Y = ?)
sec
sbc S    ; Subtract S from register A
sta T    ; Store the result into variable T
         ; cpu_state is (A = T, X = 3, Y = ?)</pre>

        <p>Lastly, the <b>return operation</b>, which is implemented with a single RTS instruction.
        This one is so simple, I'm not going to illustrate it with a code example.
        Simply imagine a "RTS" appended onto the end of the previous two combinations.
        </p>

        <h4>Finishing up</h4>

        <p>Once the combinations have been run, the algorithm selects the lowest-costing one.
        In this case, it's the one that uses a SAX instruction instead of AND,
        as it requires one less instruction.</p>

        <p>The resulting selection looks like this, but we're not done yet:</p>

        <pre>
jsr fn
sta Q
eor #5
sta R
ldx #3
sax S
sec
sbc S
sta T
rts</pre>

        <p>The issue is, most of these store instructions are unecessary, as nothing will ever load them.
        To fix this, an extra pass is run which identifies stores that are never loaded.
        This results in the final code:</p>

        <pre>
jsr fn
eor #5
ldx #3
sax S
sec
sbc S
rts</pre>

        <p>In the actual compiler, the code generator is a bit smarter about stores,
        and can usually estimate if they're needed or not while generating the combinations.
        This is important, as the presence of a store influences a combination's cost.
        </p>

        <h3>Basic Block Instruction Selection - Part 3: Continuations</h3>

        <p>As shown, compiling an IR operation boils down to generating a bunch of combinations.
        However, if you were to go in and implement each combination for each operation,
        you'd find it to be a repetive, menial coding task.
        </p>

        <p>The issue is, our combinations have combinations of their own. 
        For example, to generate a SAX instruction, one value needs to be loaded into register A,
        another into register X, followed by the SAX instruction.
        But as there is more than one way to load a value into a register 
        (including different addressing modes), 
        there will be a few combinations for loading in A, 
        multiplied by a few combinations for loading in X. </p>

        <img src="codegen/xzibit.jpg">

        <p>It would be nice to abstract these details away and create building blocks for 
        implementing operations. Luckily, I found a solution by using continuations.</p>

        <p>Each step (or "building block") will be implemented as a function written in 
        <a href="https://en.wikipedia.org/wiki/Continuation-passing_style">continuation-passing style</a>.
        Instead of returning, these functions will call their continuations with each instruction they want to test.
        For example, to generate three combinations, the continuation will be called three times with three different instructions.
        </p>

        <p>Here's how a function for loading a value in register A might look, in pseudocode:</p>

        <pre>
load_A(desired_value, cpu_state, continuation)
    if cpu_state.A == desired_value
        continuation(NOP)
    else if cpu_state.X == desired_value
        continuation(TXA)
    else if cpu_state.Y == desired_value
        continuation(TYA)
    else
        continuation(LDA)
        continuation(LAX)</pre>

        <p>
        If the value is already present in one of the registers: either nothing happens (NOP), or a transfer between registers occurs (TXA, TYA).
        Otherwise, two combinations will occur: one using the LDA instruction, 
        and another using the LAX instruction.
        </p>

        <p>The point of writing functions like this is that they can be composed.
        With a bit of plumbing, it's possible to end up with a 
        <a href=https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific language</a> that resembles:</p>

        <pre>
compose(load_A(L), load_X(R), instruction(SAX))</pre>

        <p>This single line could cover 50 different assembly sequences, all handling different combinations of instructions and addressing modes.
        It's much easier to write than trying to enumerate all the possibilities by hand.</p>

        <p>Unfortunately, I'm going to the leave implementation of this vague, as it depends on the language used.
        In my case, I was using C++, and implemented the continuations as <a href="https://en.wikipedia.org/wiki/Template_(C%2B%2B)">templates</a>.</p>

        <h3>Control Flow - Part 1: Basics</h3>

        <p>So far, the algorithm I've described is tied to basic blocks and does not handle control flow (branches, loops, etc), 
        but it's easy to extend.
        It turns out that branch operations can be handled like any other operation 
        &mdash; just create combinations of instructions and prune &mdash; using labels decided in advance.
        </p>

        <p>For example, take a look at this control flow graph (the rectangles are basic blocks):</p>

        <img src="codegen/cfg.png">

        <p>Each basic block in this graph will be assigned a unique label name (L1, L2, L3, etc).</p>

        <p>Now we'll generate the code for each basic block separately, implementing branches as instructions that work with these label names.
        Once every basic block has its code generated, we can concatenate the results:</p>

        <pre>
L1:
    ; (More code would go here)
    beq L1 ; Branch instruction
    bne L2
L2:
    ; (More code would go here)
    jmp L4 ; Jump instruction
L3:
    ; (More code would go here)
    beq L1
    bne L4
L4:
    ; (More code would go here)
    rts</pre>

        <p>With this implemented, the compiler now works. 
        It can generate executables!</p>

        <h3>Control Flow - Part 2: Optimizations</h3>

        <p>The process above works, but does not produce good code across basic block boundaries.
        The issue stems from compiling basic blocks separately without sharing knowledge of the register states between them.
        If one basic block needs a value from a previous block, it will emit a load instruction 
        even if it's redundant.</p>

        <p>For example, this is how a loop would look:</p>

        <pre>
L0:
    ldx #0 ; Load register X with 0
    stx I  ; Store variable I
L1:
    ldx I  ; Load variable I
    inx    ; Increment register X
    stx I  ; Store variable I
    bne L1 ; Branch if register X is not zero</pre>

        <p>But this is how a loop <i>should</i> look:</p>

        <pre>
    ldx #0 ; Load register X with 0
L1:
    inx    ; Increment register X
    bne L1 ; Branch if register X is not zero</pre>

        <p>The second loop lacks redundant loads.</p>

        <p>To solve this, the final "cpu_state" from basic blocks need to be shared.
        This can be done by passing each basic block's result into its successors as input.
        </p>

        <img src="codegen/prop1.png">
        <br><sup>Old method: Generate code for each basic block individually.</sup>

        <img src="codegen/prop2.png">
        <br><sup>New method: Use the output states from generating a basic block as the input states to the next.</sup>

        <p>This is an iterative process where each node can be processed multiple times.
        When a node is processed, it will trigger its successors to be processed next. This will repeat until a fixed point is reached.
        </p>

        <p>Going back to the loop example, initial iterations will produce inefficient code,
        but later iterations will propagate the register states through the loop from (L3) to (L1).
        Once variable "I" appears pre-loaded in register X, optimal code will follow.</p>

        <p>To expedite this process, it's beneficial to only pass the lowest-costing results along 
        and get stricter each iteration.
        This resembles <a href="https://en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</a>.
        </p>

        <h3>Control Flow - Part 3: Some Loads Required</h3>

        <p>Although the method above generates better code, 
        it's no longer possible to get a working program by concatenating the lowest-costing combinations.
        The issue is, each code combination now expects some specific register values as inputs,
        but the output of one combination may not correspond to the input of another.
        You could get nonsensical results like initializing a loop variable using the Y register, 
        but incrementing using the X register:</p>

        <pre>
; This code is incorrectly compiled!
L0:
    ldy #0   ; Output Y as the iteration count
             ; NOTE: A sty instruction was removed by the "remove unused stores" pass.
L1:
             ; Input X as the iteration count
    inx
    bne L1</pre>

        <p>To fix this, the input states of each basic block must match the output states of their predecessors.
        If there is a discrepency, a load can be inserted to correct it.</p>

        <p>By inserting loads, the code becomes:</p>

        <pre>
L0:
    ldy #0
    sty I   ; This store is no longer removed.
L1_entry:
    ldx I   ; Inserted load
L1:
    inx
    bne L1</pre>

        <p>Which works, but isn't ideal.</p>

        <h3>Control Flow - Part 4: PBQP</h3>

        <p>The woes above are caused by selecting only the lowest-costing combinations,
        without taking into consideration the compatability between input/output states.
        It would be better to factor in the cost of these loads, so that the sum of
        the chosen combinations with the extra loads is minimized. </p>

        <p>If this sounds like an optimization problem, that's because it is.
        It's called the <a href="https://beza1e1.tuxen.de/articles/pbqp.html">Partitioned Boolean Quadratic Problem</a>, 
        or PBQP for short.</p>

        <p>Unfortunately, not much has been written on PBQP, and what does exist is hard to read:</p>
        <img src="codegen/pbqp.png">
        <br><sup><a href="http://www.complang.tuwien.ac.at/scholz/pbqp.html">Image Source</a></sup>
        <p>My advice is that instead of struggling over equations, it's easier to understand PBQP by thinking visually, in terms of graphs.
        </p>

        <p>Imagine a graph where each node has a finite list of choices,
        and each of these choices has a cost.
        The goal PBQP is to pick one choice for each node, minimizing the total cost.
        The catch is, an additional cost is incurred for every edge in the graph
        based on the choices made at the edge's vertices.
        </p>

        <p>The picture below illustrates such a graph. 
        Each node has a choice of two colors, and those colors have a cost next to them.
        When selecting two colors, the cost of the edge must be paid as well.
        This is done by looking up the cost in a table unique to each edge, using the two selected colors as keys.
        </p>

        <img src="codegen/pbqp2.png">

        <p>Below, I've made some choices, and have crossed out everything I didn't choose.
        The total cost is the sum of the visible numbers, which is 3 + 1 + 7 + 3 + 6 + 7 = 27.
        The goal of PBQP is to find choices which minimizes this sum.</p>

        <img src="codegen/pbqp3.png">

        <p>A <a href="https://pp.ipd.kit.edu/uploads/publikationen/mbaumstark19bachelorarbeit.pdf">fast algorithm</a>
        exists to solve PBQP problems near-optimally.
        The algorithm works on the graph, reducing and combining nodes until none remain. 
        At the end, the algorithm works backwards, using the steps it took to decide which choices to make.</p>

        <p>Going back to the compiler, the problem I was describing fits perfectly into PBQP:</p>
        <ul>
            <li>The graph is the control-flow graph.</li>
            <li>The nodes are the basic blocks.</li>
            <li>The node choices are the code combinations generated prior.</li>
            <li>The edge costs are the number of extra loads that have to be inserted given two code combinations.</li>
        </ul>

        <p>By implementing a PBQP solver and using it on our combinations, the end result will produce near-optimal code.
        The assembly loop which gave us trouble before will become:</p>

<pre>
    ldx #0
L1:
    inx
    bne L1</pre>

        <p>Which is optimal.</p>

        <h3>Peep-Hole and Other Cleanup</h3>

        <p>The generated assembly code should be high quality at this point, 
        but a few small gains can still be had using 
        <a href="https://en.wikipedia.org/wiki/Peephole_optimization">peep-hole optimization</a>.
        Mostly, this is done for the loads inserted at the very end (between basic-block transitions),
        as those weren't generated using the combination method, and so lack the optimization the surrounding code has.
        </p>

        <p>In addition to peephole, it's recommended to clean up the branch code a bit.
        In my own implementation, I generate instructions for both destinations of the branch, and then remove one in this cleanup pass.</p>

        <p>There can also be some benefit to reordering instructions. 
        Sometimes, this is useful because the operation order was not optimal,
        but other times it's useful because the combination algorithm places loads right before they're used.
        Both of these are hard to do, so my compiler only reorders loads, 
        moving them based on liveness information and pattern matching.</p>

        <h2>Complexity and Performance</h2>

        <ul>
            <li>Creating the order is O(N<sup>2</sup>), but has a negligable runtime cost.
            In practice, typical IRs are very easy to order.</li>
            <li>I don't know what the complexity of going out of SSA is, but I'm assuming its O(N<sup>2</sup>). 
                    Regardless, the runtime cost is negligable.</li>
            <li>Code-generating basic blocks is O(N), with a high constant multiplier.
                This has an impactful runtime cost, with the bottleneck being the hash table.
                I use a very fast hash-table of my own design (amusingly, it's faster than the so-called <a href="https://probablydance.com/2017/02/26/i-wrote-the-fastest-hashtable/">"fastest hash table"</a>), which alleviates this.
            </li>
            <li>The PBQP problem is solved in O(N) relative to the number of edges, and O(N<sup>3</sup>) relative to the number of choices.
                This <i>can</i> be a big runtime cost, but isn't an issue so long as the number of choices is kept small.
                I reckon the PBQP solver could be implemented using SIMD if necessary, but I haven't found the need.</li>
        </ul>

        <p>Generating combinations easily fills the cache, and once generated, 
        there's a ton of cache misses involved in reading the combinations back.
        Thus, it's a good idea to evaluate and discard the combinations in a manner that doesn't require </p>

        TODO!!!

        <p>Rather than tracking memory precisely, 
        I allocate combinations in a hefty memory pool and free them after each step.
        I did experiment with a few garbage-collection ideas, but none proved fruitful.</p>

        <p>Coming up with a decent implementation of continuations in C++ was challenging.
        The issue was, either compilation speed would blow up, or run-time speed would blow up, or both.
        Eventually, I settled on using templates to generate linked-lists of functions, which are then evaluated.
        This was the fastest at both compile-time and run-time, but it's difficult for me to assess why.
        </p>

        <h3>Control Flow - Part 3: PBQP</h3>

        <h2>Other Approaches</h2>

        <p>As I mentioned in the beginning, I know very little about </p>

        <p>This article is really about code-generation, but I'll provide a </p>

        <h2>Unfair Advantages</h2>

        <p>
            I really don't like 

            The first benefit my compiler has is that I'm using the so-called "illegal" (or "unofficial") instructions.
        </p>

        <img src="codegen/phi1.png">
        <img src="codegen/phi2.png">
        <img src="codegen/phi3.png">


        <p><a href="index.html#softwarearticles">[More programming articles]</a> <a href="index.html">[Back to my homepage]</a></p>
    </body>
</html>
